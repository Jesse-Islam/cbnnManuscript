---
title: "oldmortality"
output: html_document
params:
  epo: 2000
  patience: 10
  iteration: 1
  layer1: 200
  layer2: 20
  layer3: 25
  layer4: 25
  drpt: 0.1
---



```{r,eval=T}

library(reticulate)
```

```{python,eval=T}

import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID" 
os.environ["CUDA_VISIBLE_DEVICES"] = ""
#os.environ["OM_NUM_THREADS"]="2"

```



```{r setup, include=T}
set.seed(1)
knitr::opts_chunk$set(echo = TRUE)
source("src/packages.R")
source("src/functions.R")
eval_metabric<-T
pycox<-reticulate::import("pycox")
source_python("src/cIndex.py")
source_python("src/dsmScript.py")
source_python('src/oldmortDeephitter.py')
##########################
###Shared Hyperparameters
##########################
bsize=512
min_delta = 10^-9
epo=as.numeric(params$epo)
patience = as.numeric(params$patience)
iteration=as.numeric(params$iteration)
layer1<-as.numeric(params$layer1)
layer2<-as.numeric(params$layer2)
layer3<-as.numeric(params$layer3)
layer4<-as.numeric(params$layer4)
drpt<-as.numeric(params$drpt)

```




```{r testAnalysis}

start_time <- Sys.time()
BrierIPAList<-list()
cScore<-list()
library(eha)

data("oldmort")

preData<-data.frame(sex=oldmort$sex,
                 civ=oldmort$civ,
                 ses=oldmort$ses.50,
                 birthplace=oldmort$birthplace,
                 ageAtEntry=oldmort$enter,
                 imrAtBirth=oldmort$imr.birth,
                 region=oldmort$region,
                 birthdate=oldmort$birthdate,
                 time=oldmort$exit-oldmort$enter,
                 status=as.numeric(oldmort$event))

#preData$time<-preData$time+runif(nrow(preData),-1,1)/1000000




data<-as.data.frame((model.matrix(~.,preData)[,-1]))
data<-as.data.frame(sapply(data, function(x) (x - min(x, na.rm = T)) / (max(x, na.rm = T) - min(x, na.rm=T))))
covs<-colnames(data)

samp<-sample(seq(1,nrow(data),by=1), floor(nrow(data)*.80)) 
trainOG<-data[samp,]
test<-data[-samp,-c(ncol(data))]

fullTest<-data[-samp,,drop=F]




pos<-1

while (pos <= iteration) {
  

  
  samp<-sample(seq(1,nrow(trainOG),by=1) , nrow(trainOG),replace = T) 
  train<-trainOG[samp,]
  times<-seq(from=min(test$time),
             to=max(test$time),
             length.out = 100
  )
  times<-times[-c(1,length(times))]
  
  ######################
  ###casebase + splines
  ######################
  mod_cb_glm <- fitSmoothHazard(status~bs(time)+.-time,
                                data = train,
                                time = "time",
                                event="status",
                                ratio = 100
  )
  
  glmAbsRisk<-as.data.frame(absoluteRisk(mod_cb_glm,time=times,newdata=test,type="CI"))
  rownames(glmAbsRisk)<-glmAbsRisk$time
  #prepare to run through riskRegression
  glmProper<- t(glmAbsRisk[-1,-1])
  class(glmProper)<-c("tunnel",class(glmProper))
  
  ######################
  ###DeepSurv
  ######################

  coxnn<-deepsurv(formula=Surv(time, status) ~ .,data = train, frac = 0.2, activation = 'selu',
                  num_nodes = c(layer1,layer2), dropout = drpt,
                  early_stopping = TRUE, epochs = epo,
                  batch_size = bsize,device = "cpu",best_weights=T,
                  patience = patience,min_delta=min_delta,batch_norm = T,
                  learning_rate = 0.0001,lr_decay=10^-7)
  coxnnsurv<- predict(coxnn,newdata = test,type="survival")
  #colnames(coxnnsurv)<-seq(0,1,length.out = ncol(coxnnsurv))
  cnnCleaned<-pyProcess(coxnnsurv,times=times)
  colnames(cnnCleaned)<-seq(0,1,length.out = ncol(cnnCleaned))
  
  # source_python('src/deepsurver.py')
  # coxnnsurv=fitDeepSurv(train,fullTest,bsize,epochs=epo,patience=patience,min_delta=min_delta,drpt=drpt)
  # coxnnsurv<-t(coxnnsurv)
  # #colnames(coxnnsurv)<-seq(0,1,length.out = ncol(coxnnsurv))
  # cnnCleaned<-pyProcess(coxnnsurv,times=times)
  # colnames(cnnCleaned)<-seq(0,1,length.out = ncol(cnnCleaned))
  # py_run_string("del fitDeepSurv")
  # rm(fitDeepSurv)
  
  ######################
  ###Deephit
  ######################
  source_python('src/oldmortDeephitter.py')
  hitnnSurv=fitDeephit(train,fullTest,bsize,epo,patience=patience,min_delta=min_delta,drpt=drpt,lay1=layer1,lay2=layer2)
  hitnnSurv<-t(hitnnSurv)
  #colnames(hitnnSurv)<-seq(0,1,length.out = ncol(hitnnSurv))
  deephitCleaned<-pyProcess(hitnnSurv,times=times)
  #colnames(deephitCleaned)<-seq(0,1,length.out = ncol(deephitCleaned))
  py_run_string("del fitDeephit")
  rm(fitDeephit)
  
  

  #######################
  ###Cox 
  ######################
  cox<-coxph(Surv(time, status) ~ ., data = train,x=T)
  a<-survfit(cox, newdata=fullTest,times=times)
  rownames(a$surv)<-a$time
  coxlinSurv<-pyProcess(t(a$surv),times=times)
  colnames(coxlinSurv)<-seq(0,1,length.out = ncol(coxlinSurv))
  
  ######################
  ###PMNN splines
  ######################
  covars_input<-layer_input(shape=c(length(colnames(mod_cb_glm$data))-2+3),
                            name = 'main_input'
  )
  covars_output<-covars_input%>%
    layer_dense(units=layer1,use_bias = T)%>%
    activation_selu()%>%
    layer_batch_normalization()%>%
    layer_dropout(drpt)%>%
    layer_dense(units=layer2,use_bias = T)%>%
    activation_selu()%>%
    layer_batch_normalization()%>%
    layer_dropout(drpt)%>%
    layer_dense(units=1,use_bias = T)#%>%activation_tanh()

  pmnn<-pmnnModel(features=colnames(mod_cb_glm$data)[-ncol(mod_cb_glm$data)],
                  feature_input = covars_input,
                  feature_output = covars_output,
                  originalData = mod_cb_glm$data,
                  offset=mod_cb_glm$offset,
                  timeVar = "time",
                  eventVar= "status",
                  optimizer=optimizer_adam(learning_rate = 0.0001,decay=10^-7)
  )

  fit<-fitSmoothHazSpline(pmnn,
                          epochs=epo,
                          batch_size=512,
                          verbose=0,
                          monitor="val_loss",
                          val_split=0.2,
                          min_delta=min_delta,
                          patience=patience)
  annPreds<-aarSplines(fit,
                       times=times,
                       x_test=test
  )
  rownames(annPreds)<-annPreds[,1]
  annProper<- t(annPreds[,-1])
  class(annProper)<-c("tunnel",class(annProper))

  rm(pmnn,covars_output,covars_input,fit)



  
  
  ######################
  ###PMNN
  ######################
  covars_input<-layer_input(shape=c(length(colnames(mod_cb_glm$data))-2),
                            name = 'main_input')
  
  covars_output<-covars_input%>%
    layer_dense(units=layer1,use_bias = T)%>%
    activation_selu()%>%
    layer_batch_normalization()%>%
    layer_dropout(drpt)%>%
    
    layer_dense(units=layer2,use_bias = T)%>%
    activation_selu()%>%
    layer_batch_normalization()%>%
    layer_dropout(drpt)%>%
    layer_dense(units=1,use_bias = T)#%>%activation_tanh()
  
  pmnn<-pmnnModel(features=colnames(mod_cb_glm$data)[-ncol(mod_cb_glm$data)],
                  feature_input = covars_input,
                  feature_output = covars_output,
                  originalData = mod_cb_glm$data,
                  offset=mod_cb_glm$offset,
                  timeVar = "time",
                  eventVar= "status",
                  optimizer=optimizer_adam(learning_rate = 0.0001,decay=10^-7)
  )
  
  fit<-fitSmoothHaz(pmnn,
                    epochs=epo,
                    batch_size=bsize,
                    verbose=0,
                    monitor="val_loss",
                    val_split=0.2,
                    min_delta=min_delta,
                    patience=patience)
  annPreds<-aar(fit,
                times=times,
                x_test=test
  )
  
  rownames(annPreds)<-annPreds[,1]
  annProperpoly<- t(annPreds[,-1])
  class(annProperpoly)<-c("tunnel",class(annProperpoly)) 
  
  
  
  ######################
  ###IPA metric
  ######################
  brierFinalResults <- Score(list("Cox_Lin" = cox,'CB_Logi'=glmProper,
                                  'DeepSurv'=cnnCleaned,'DeepHit'=deephitCleaned,
                                  'PMNN_Spline'=annProper,'PMNN_Poly'=annProperpoly),#
                             data =fullTest, 
                             formula = Hist(time, status != 0) ~ 1, summary = c("risks","IPA","ibs"), 
                             se.fit = FALSE, metrics = "brier", contrasts = FALSE, times = times)
  BrierIPAList[[pos]]<-brierFinalResults$Brier$score
  
  ggplot(data=brierFinalResults$Brier$score, aes(x=times,y=IPA,col=model))+
  geom_line()+coord_cartesian(ylim=c(-0.5,0.7))
  rm(pmnn,covars_output,covars_input,fit)
  
  
  

  
  
  ######################
  ###C-Index
  ######################

  riskList<-list(coxlinSurv,
                 glmProper,
                 cnnCleaned,
                 deephitCleaned,
                 annProper,
                 annProperpoly
  )
  

  tempTims<-   as.numeric(colnames(annProper))
  tempTims<-head(tempTims, -1)
  tempTims<-tail(tempTims, -1)
  cScoreTemp<-matrix(NA,nrow=length(tempTims),ncol=length(riskList)+1)
  et_train<-as.matrix(train)[,c(16,15)]
  et_test<-as.matrix(fullTest[,c(16,15)])
  colnames(cScoreTemp)<-c('times', "Cox_Lin",'CB_Logi','DeepSurv','DeepHit',
                          'PMNN_Spline','PMNN_Poly')
  cScoreTemp[,1]<- tempTims

  cScore[[pos]]<-cIndexSummary(et_train=et_train,
                               et_test=et_test,
                               riskList=riskList,
                               cScore=cScoreTemp
  )
  
  saveRDS(BrierIPAList,'results/100oldBrier.rds')
  saveRDS(cScore,'results/100oldcidx.rds')
  pos<-pos+1
}






ggplot(data=brierFinalResults$Brier$score, aes(x=times,y=Brier,col=model))+
  geom_line()
ggplot(data=brierFinalResults$Brier$score, aes(x=times,y=IPA,col=model))+
  geom_line()

end_time <- Sys.time()

end_time-start_time


```



```{r tuning,eval=F}
library(kerastuneR)
  ######################
  ###PMNN
  ######################
samp<-sample(seq(1,nrow(trainOG),by=1), floor(nrow(trainOG)*.80)) 
validation<-trainOG[-samp,]

train<-trainOG[samp,]

  mod_cb_glm <- fitSmoothHazard(status~bs(time)+.-time,
                                data = train,
                                time = "time",
                                event="status",
                                ratio = 100
  )


buildModel<-function(hp){
  covars_input<-layer_input(shape=c(length(colnames(mod_cb_glm$data))-2),
                            name = 'main_input')
  
  covars_output<-covars_input%>% layer_batch_normalization()%>%
    layer_dense(units=hp$Int('units',min_value=5,max_value=25,step=2),
                use_bias = T,activation = 'linear')%>%
    layer_dropout(hp$Choice('rate',
                values=c(0.4,0.3,0.2,0.1,0.05)))%>%
    layer_dense(units=hp$Int('units',min_value=5,max_value=25,step=2),
                use_bias = T,activation = 'linear')%>%
    layer_dropout(hp$Choice('rate',
                values=c(0.4,0.3,0.2,0.1,0.05)))%>%
    layer_dense(units=1,use_bias = T)
  
  pmnn<-pmnnModel(features=colnames(mod_cb_glm$data)[-ncol(mod_cb_glm$data)],
                  feature_input = covars_input,
                  feature_output = covars_output,
                  originalData = mod_cb_glm$data,
                  offset=mod_cb_glm$offset,
                  timeVar = "time",
                  eventVar= "status",
                  optimizer= tf$keras$optimizers$Adam(
      hp$Choice('learning_rate',
                values=c(1e-2,5e-3, 1e-3,5e-4, 1e-4)))
  )
  return(pmnn$model)
}
  
  tuner = RandomSearch(
    buildModel,
    objective = 'val_loss',
    max_trials = 20,
    executions_per_trial = 3,
    directory = 'tuning',
    project_name = 'oldmort')
  
  tuner %>% search_summary()
  
  xtr<-as.matrix(mod_cb_glm$data[,-c(ncol(mod_cb_glm$data),ncol(mod_cb_glm$data)-1)])
  ytr<-as.matrix(mod_cb_glm$data[,c(ncol(mod_cb_glm$data)-1)])
  xv<-list(as.matrix(validation[,-c(ncol(validation))]),as.matrix(data.frame(offset=rep(0,nrow(validation)))))
  yv<-as.matrix(validation[,c(ncol(validation)),drop=F])
  tuner %>% fit_tuner(x=list(xtr,as.matrix(mod_cb_glm$offset)),y=ytr,
                    epochs = 50,batch_size=bsize+1000, 
                    validation_data = list(x=xv,y=yv),verbose=0 )
result = kerastuneR::plot_tuner(tuner)
# the list will show the plot and the data.frame of tuning results
result 

best_5_models = tuner %>% get_best_models(5)
best_5_models[[1]] %>% plot_keras_model()
  # 
  # fit<-fitSmoothHaz(pmnn,
  #                   epochs=epo,
  #                   batch_size=bsize+1000,
  #                   verbose=0,
  #                   monitor="val_loss",
  #                   val_split=0.2,
  #                   min_delta=min_delta,
  #                   patience=patience+10)
  # annPreds<-aar(fit,
  #               times=times,
  #               x_test=test
  # )
  # 
  # rownames(annPreds)<-annPreds[,1]
  # annProperpoly<- t(annPreds[,-1])
  # class(annProperpoly)<-c("tunnel",class(annProperpoly)) 
  # 

```