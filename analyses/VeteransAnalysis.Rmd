---
title: "vets"
output: html_document
params:
  epo: 2000
  patience: 10
  iteration: 1
  layer1: 50
  layer2: 20
  layer3: 25
  layer4: 25
  drpt: 0.1
---



```{r,eval=T}

library(reticulate)
```

```{python,eval=T}

import os
os.environ["CUDA_DEVICE_ORDER"] = "PCI_BUS_ID" 
os.environ["CUDA_VISIBLE_DEVICES"] = ""
#os.environ["OM_NUM_THREADS"]="2"

```



```{r setup, include=T}
set.seed(1)
knitr::opts_chunk$set(echo = TRUE)
source("src/packages.R")
source("src/functions.R")
eval_metabric<-T
pycox<-reticulate::import("pycox")
source_python("src/cIndex.py")
source_python("src/dsmScript.py")
source_python('src/oldmortDeephitter.py')
##########################
###Shared Hyperparameters
##########################
bsize=512
min_delta = 10^-8
epo=as.numeric(params$epo)
patience = as.numeric(params$patience)
iteration=as.numeric(params$iteration)
layer1<-as.numeric(params$layer1)
layer2<-as.numeric(params$layer2)
layer3<-as.numeric(params$layer3)
layer4<-as.numeric(params$layer4)
drpt<-as.numeric(params$drpt)

```




```{r testAnalysis}

start_time <- Sys.time()
BrierIPAList<-list()
cScore<-list()
#library(eha)
veteran<-veteran

coxph(Surv(time, status) ~ trt + prior + karno, veteran)


preData<-data.frame(trt=veteran$trt,
                    prior=veteran$prior,
                    karno=veteran$karno,
                    time=veteran$time,
                    status=veteran$status
                    )
#preData$time<-preData$time+runif(nrow(preData),0,1)/100000000

data<-as.data.frame((model.matrix(~.,preData)[,-1]))

data<-as.data.frame(sapply(data, function(x) (x - min(x, na.rm = T)) / (max(x, na.rm = T) - min(x, na.rm=T))))
covs<-colnames(data)

#samp<-sample(seq(1,nrow(data),by=1), floor(nrow(data)*.80)) 





 spec = c(train = .90, test = .10)#, validate = 0)
 
 g = sample(cut(
   seq(nrow(data)), 
   nrow(data)*cumsum(c(0,spec)),
   labels = names(spec)
 ))
 
 res = split(data, g)


trainOG<-res$train

test<-res$test[,-c(ncol(data))]

fullTest<-res$test

val<-res$validate

valcb<-sampleCaseBase(data = trainOG,time="time",event="status",ratio=100)
valcb$offset<-0
valcbtensor<-list(list(as.matrix(valcb[,-c(ncol(valcb)-1,ncol(valcb))]),as.matrix(valcb[,ncol(valcb),drop=F])),as.matrix(valcb[,ncol(valcb)-1,drop=F]))
 #valcbtensor<-list(list(as.matrix(val[,-c(ncol(val))]),  matrix(0,ncol=1,nrow=nrow(val))), as.matrix(val[,c(ncol(val))]) )
pos<-1

while (pos <= iteration) {
  

  
  samp<-sample(seq(1,nrow(trainOG),by=1) , nrow(trainOG),replace = T) 
  train<-trainOG[samp,]
  times<-seq(from=min(test$time),
             to=max(test$time),
             length.out = 200
  )
  times<-times[-c(1,length(times))]
  
  print('starting cbspline')
  ######################
  ###casebase + splines
  ######################
  mod_cb_glm <- fitSmoothHazard(status~bs(time)+.-time,
                                data = train,
                                time = "time",
                                event="status",
                                ratio = 100
  )
  
  glmAbsRisk<-as.data.frame(absoluteRisk(mod_cb_glm,time=times,newdata=test,type="CI"))
  rownames(glmAbsRisk)<-glmAbsRisk$time
  #prepare to run through riskRegression
  glmProper<- t(glmAbsRisk[-1,-1])
  class(glmProper)<-c("tunnel",class(glmProper))
  
  
   print('starting deepsurv')
  ######################
  ###DeepSurv
  ######################
#   coxnn<-deepsurv(formula=Surv(time, status) ~ .,data = train, frac = 0.2, activation = 'tanh',
#                   num_nodes = c(layer1,layer2), dropout = drpt,
#                   early_stopping = TRUE, epochs = epo,
#                   batch_size = bsize,device = "cpu",best_weights=T,
#                   patience = patience,min_delta=min_delta,batch_norm = T,
#                   learning_rate = 0.0001,lr_decay=10^-7)
#   coxnnsurv<- predict(coxnn,newdata = test,type="survival")
#   colnames(coxnnsurv)<-seq(0,1,length.out = ncol(coxnnsurv))
#   cnnCleaned<-pyProcess(coxnnsurv,times=times)
#   colnames(cnnCleaned)<-seq(0,1,length.out = ncol(cnnCleaned))
  
  source_python('src/deepsurver.py')
  #need to break ties
  tiebreakTrain<-train
  
  #while(length(unique(tiebreakTrain$time))/length(tiebreakTrain$time)!=1 ){
  #tiebreakTrain<-train
  #tiebreakTrain$time<-tiebreakTrain$time+runif(nrow(tiebreakTrain),0,1/10000000000)
  #}
  
  coxnnsurv=fitDeepSurv(tiebreakTrain,fullTest,bsize,epochs=epo,valida=val,patience=patience,min_delta=min_delta,drpt=drpt)
  coxnnsurv<-t(coxnnsurv)
  #colnames(coxnnsurv)<-seq(0,1,length.out = ncol(coxnnsurv))
  cnnCleaned<-pyProcess(coxnnsurv,times=times)
  colnames(cnnCleaned)<-times#seq(0,1,length.out = ncol(cnnCleaned))
  py_run_string("del fitDeepSurv")
  rm(fitDeepSurv)
  
   print('starting deephit')
  ######################
  ###Deephit
  ######################
  source_python('src/oldmortDeephitter.py')
  hitnnSurv=fitDeephit(train,fullTest,bsize,epo,valida=val,patience=patience,min_delta=min_delta,drpt=drpt)
  hitnnSurv<-t(hitnnSurv)
  #colnames(hitnnSurv)<-seq(0,1,length.out = ncol(hitnnSurv))
  deephitCleaned<-pyProcess(hitnnSurv,times=times)
  colnames(deephitCleaned)<-times#seq(0,1,length.out = ncol(deephitCleaned))
  py_run_string("del fitDeephit")
  rm(fitDeephit)
  
  
   print('starting cbnnsplines')
  ######################
  ###PMNN splines
  ######################
  ###########################################
###########################################
  valcbtensor[[1]][[2]]<-rep(mod_cb_glm$offset[1],nrow(valcbtensor[[1]][[1]]))
###########################################  
###########################################  
  covars_input<-layer_input(shape=c(length(colnames(mod_cb_glm$data))-2+3),
                            name = 'main_input'
  )
  covars_output<-covars_input%>%
    layer_dense(units=layer1,use_bias = T)%>%
    layer_batch_normalization()%>%
    activation_tanh()%>%
    layer_dropout(drpt)%>%
    
    layer_dense(units=layer2,use_bias = T)%>%
    layer_batch_normalization()%>%
    activation_tanh()%>%
    layer_dropout(drpt)%>%
    layer_dense(units=1,use_bias = T)#%>%activation_tanh()

  pmnn<-pmnnModel(features=colnames(mod_cb_glm$data)[-ncol(mod_cb_glm$data)],
                  feature_input = covars_input,
                  feature_output = covars_output,
                  originalData = mod_cb_glm$data,
                  offset=mod_cb_glm$offset,
                  timeVar = "time",
                  eventVar= "status",
                  optimizer=optimizer_adam(learning_rate = 0.001)#,decay=10^-7)
  )

  fit<-fitSmoothHazSpline(pmnn,
                          epochs=epo,
                          batch_size=bsize,
                          verbose=0,
                          monitor="val_loss",
                          #val_split=0.2,
                          min_delta=min_delta,
                          patience=patience,val=valcbtensor)
  annPreds<-aarSplines(fit,
                       times=times,
                       x_test=test
  )
  rownames(annPreds)<-annPreds[,1]
  annProper<- t(annPreds[,-1])
  class(annProper)<-c("tunnel",class(annProper))
  #ggplot(data=brierFinalResults$Brier$score, aes(x=times,y=IPA,col=model))+
  #geom_line()+coord_cartesian(ylim=c(-0.2,0.5))
  rm(pmnn,covars_output,covars_input,fit)

 print('starting cox')
  ######################
  ###coxph
  ######################
  cox<-coxph(Surv(time, status) ~ ., data = train,x=T)
  
   print('starting cbnn')
  ######################
  ###PMNN
  ######################
  covars_input<-layer_input(shape=c(length(colnames(mod_cb_glm$data))-2),
                            name = 'main_input')
  
  covars_output<-covars_input%>% layer_batch_normalization()%>%
    layer_dense(units=layer1,use_bias = T,activation = 'relu')%>%
    layer_dropout(drpt)%>%
    layer_dense(units=layer2,use_bias = T,activation = 'relu')%>%
    layer_dropout(drpt)%>%
    layer_dense(units=layer3,use_bias = T,activation = 'relu')%>%
    layer_dropout(drpt)%>%
    layer_dense(units=layer4,use_bias = T,activation = 'relu')%>%
    layer_dropout(drpt)%>%
    layer_dense(units=1,use_bias = T)
  
  pmnn<-pmnnModel(features=colnames(mod_cb_glm$data)[-ncol(mod_cb_glm$data)],
                  feature_input = covars_input,
                  feature_output = covars_output,
                  originalData = mod_cb_glm$data,
                  offset=mod_cb_glm$offset,
                  timeVar = "time",
                  eventVar= "status",
                  optimizer=optimizer_adam(learning_rate = 0.0001,decay=10^-7)
  )
  
  fit<-fitSmoothHaz(pmnn,
                    epochs=epo,
                    batch_size=bsize,
                    verbose=0,
                    monitor="loss",
                    #val_split=0.3,
                    min_delta=min_delta,
                    patience=patience,val=valcbtensor)
  annPreds<-aar(fit,
                times=times,
                x_test=test
  )
  
  rownames(annPreds)<-annPreds[,1]
  annProperpoly<- t(annPreds[,-1])
  class(annProperpoly)<-c("tunnel",class(annProperpoly)) 
  

  #current best valLoss=0.01048
   print('starting brier')
  ######################
  ###IPA metric
  ######################
  brierFinalResults <- Score(list("Cox_Lin" = cox,'CB_Logi'=glmProper,
                                  #'DeepSurv'=cnnCleaned,'DeepHit'=deephitCleaned,
                                  'PMNN_Spline'=annProper,'PMNN_Poly'=annProperpoly),#
                             data =fullTest, 
                             formula = Hist(time, status != 0) ~ 1, summary = c("risks","IPA","ibs"), 
                             se.fit = FALSE, metrics = "brier", contrasts = FALSE, times = times)
  BrierIPAList[[pos]]<-brierFinalResults$Brier$score
  
  ggplot(data=brierFinalResults$Brier$score, aes(x=times,y=IPA,col=model))+
  geom_line()+coord_cartesian(ylim=c(-1,1))
  fit$resultOfFit
  rm(pmnn,covars_output,covars_input,fit)
  
  
  ######################
  ###Cox###           ##
  ######################
  a<-survfit(cox, newdata=fullTest,times=times)
  rownames(a$surv)<-a$time
  coxlinSurv<-pyProcess(t(a$surv),times=times)
  colnames(coxlinSurv)<-times#seq(0,1,length.out = ncol(coxlinSurv))
  
   print('starting cindex')
  ######################
  ###C-Index
  ######################

  riskList<-list(coxlinSurv,
                 glmProper,
                 cnnCleaned,
                 deephitCleaned,
                 annProper,
                 annProperpoly
  )
  

  tempTims<-   as.numeric(colnames(annProper))
  tempTims<-head(tempTims, -1)
  tempTims<-tail(tempTims, -1)
  cScoreTemp<-matrix(NA,nrow=length(tempTims),ncol=length(riskList)+1)
  et_train<-as.matrix(train)[,c(15,16)]
  et_test<-as.matrix(fullTest[,c(15,16)])
  colnames(cScoreTemp)<-c('times', "Cox_Lin",'CB_Logi','DeepSurv','DeepHit',
                          'PMNN_Spline','PMNN_Poly')
  cScoreTemp[,1]<- tempTims

  cScore[[pos]]<-cIndexSummary(et_train=et_train,
                               et_test=et_test,
                               riskList=riskList,
                               cScore=cScoreTemp
  )
  
 saveRDS(BrierIPAList,'results/100oldBrier.rds')
  saveRDS(cScore,'results/100oldcidx.rds')
  print(pos)
  pos<-pos+1
}






ggplot(data=brierFinalResults$Brier$score, aes(x=times,y=Brier,col=model))+
  geom_line()
ggplot(data=brierFinalResults$Brier$score, aes(x=times,y=IPA,col=model))+
  geom_line()

end_time <- Sys.time()

end_time-start_time


```



```{r tuning,eval=F}
library(kerastuneR)
  ######################
  ###PMNN
  ######################
#samp<-sample(seq(1,nrow(trainOG),by=1), floor(nrow(trainOG)*.80)) 
#validation<-trainOG[-samp,]

#train<-trainOG[samp,]

  #mod_cb_glm <- fitSmoothHazard(status~bs(time)+.-time,
  #                              data = train,
  #                              time = "time",
  #                              event="status",
  #                              ratio = 100
  #)


buildModel<-function(hp){
  covars_input<-layer_input(shape=c(length(colnames(mod_cb_glm$data))-2),
                            name = 'main_input')
  
  covars_output<-covars_input%>% layer_batch_normalization()%>%
    layer_dense(units=hp$Int('units1',min_value=50,max_value=500,step=25),
                use_bias = T,activation = 'tanh')%>%
    layer_dropout(hp$Choice('rate1',
                values=c(0.4,0.3,0.2,0.1,0.05)))%>%
    layer_dense(units=hp$Int('units2',min_value=50,max_value=500,step=25),
                use_bias = T,activation = 'tanh')%>%
    layer_dropout(hp$Choice('rate2',
                values=c(0.4,0.3,0.2,0.1,0.05)))%>%
    layer_dense(units=1,use_bias = T)
    
    
  pmnn<-pmnnModel(features=colnames(mod_cb_glm$data)[-ncol(mod_cb_glm$data)],
                  feature_input = covars_input,
                  feature_output = covars_output,
                  originalData = mod_cb_glm$data,
                  offset=mod_cb_glm$offset,
                  timeVar = "time",
                  eventVar= "status",
                  optimizer=optimizer_adam(learning_rate = 0.0001)#,decay=10^-7)
  )
  return(pmnn$model)
}



  tuner = RandomSearch(
    buildModel,
    objective = 'val_loss',
    max_trials = 20,
    executions_per_trial = 5,
    directory = 'tuning',
    project_name = 'oldmort')
  
  tuner %>% search_summary()
  
  xtr<-as.matrix(mod_cb_glm$data[,-c(ncol(mod_cb_glm$data),ncol(mod_cb_glm$data)-1)])
  ytr<-as.matrix(mod_cb_glm$data[,c(ncol(mod_cb_glm$data)-1)])
  xv<-valcbtensor[[1]]#list(as.matrix(validation[,-c(ncol(validation))]),as.matrix(data.frame(offset=rep(0,nrow(validation)))))
  yv<-valcbtensor[[2]]#as.matrix(validation[,c(ncol(validation)),drop=F])
  tuner %>% fit_tuner(x=list(xtr,as.matrix(mod_cb_glm$offset)),y=ytr,
                    epochs = 55,batch_size=bsize, 
                    validation_data = list(xv,yv),verbose=0 )
result = kerastuneR::plot_tuner(tuner)
# the list will show the plot and the data.frame of tuning results
result 

best_5_models = tuner %>% get_best_models(5)
best_5_models[[1]] %>% plot_keras_model()
  # 
  # fit<-fitSmoothHaz(pmnn,
  #                   epochs=epo,
  #                   batch_size=bsize+1000,
  #                   verbose=0,
  #                   monitor="val_loss",
  #                   val_split=0.2,
  #                   min_delta=min_delta,
  #                   patience=patience+10)
  # annPreds<-aar(fit,
  #               times=times,
  #               x_test=test
  # )
  # 
  # rownames(annPreds)<-annPreds[,1]
  # annProperpoly<- t(annPreds[,-1])
  # class(annProperpoly)<-c("tunnel",class(annProperpoly)) 
  # 

```