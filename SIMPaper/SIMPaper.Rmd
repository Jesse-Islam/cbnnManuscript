---
title: "Case-Base Neural Networks: survival analysis with time-varying, higher-order interactions"
author:
- name: Jesse Islam
  num: a
- name: Maxime Turgeon
  num: b
- name: Robert Sladek
  num: c
- name: Sahir Bhatnagar
  num: d
address:
- num: a
  org: Department of Quantitative life sciences, McGill University 
- num: b
  org: Department of Statistics, University of Manitoba
- num: c
  org: Department of Human Genetics, McGill University
- num: d
  org: Department of Biostatistics, McGill University
corres: "\\email{jesse.islam@mail.mcgill.ca}"
authormark: Islam \emph{et al}.
articletype: Research article
received: 2022-04-29
revised: 2022-06-01
accepted: 2022-07-01
abstract: "In survival analysis, covariate interactions are difficult to model using regression-based methods without prior knowledge. Neural network-based survival models have been developed for data-driven interaction modeling, however, these approaches lack implementations for time-varying interactions or a flexible baseline hazard. To address this, we extend the case-base sampling framework and propose Case-Base Neural Networks (CBNN), which introduce time directly into the model, using commonly available neural network components for a simple implementation. First, we case-base sample the survival data, converting the survival time of each participant into discrete moments in time. This allows us to estimate the probability an event occurs at a given moment using a neural network to get the hazard. We compare CBNN to survival methods based on regression and neural networks in two simulations and two real data applications with a fixed neural network architecture. We report metrics for each model over follow-up time using both a proper (Index of Prediction Accuracy) and non-proper (inverse probability censoring weights-adjusted concordance index) scoring rule. A simple simulation shows CBNN outperforming the other neural network methods. The complex simulation highlights the ability of CBNN to model both a complex baseline hazard and time-varying interactions, outperforming all competitors. The first real data application shows CBNN outperforming all neural network competitors, while a second real data application shows competitive performance. CBNN has shown that it is more consistent in prediction performance across time in the simulations and real data applications compared to the competing neural network approaches."

keywords: survival analysis, machine learning, case-base, neural network
bibliography: bibfile.bib
output: 
  rticles::sim_article:
    includes:
      in_header: preamble.tex
    keep_tex: true
header-includes: 
  - \usepackage{placeins}
  - \newcommand{\sigmoid}{\mathrm{sigmoid}}
  - \renewcommand{\thefootnote}{\alph{footnote}}
---

In survival analysis, covariate interactions are difficult to model using regression-based methods without prior knowledge. Neural network-based survival models have been developed for data-driven interaction modeling, however, these approaches can not model both time-varying interactions and complex baseline hazards. To address this, we propose Case-Base Neural Networks (CBNN) which extends the case-base sampling framework which introduce time directly into the model. We case-base sample the survival data, converting the survival time of each participant into discrete moments in time. This allows us to use a feed-forward neural network that estimate the probability of an event occurring at a given moment, which can be used to estimate the hazard function. We then compare CBNN to survival methods based on regression and neural networks in two simulations and two real data applications with a fixed neural network architecture. We report metrics for each model over follow-up time using both proper (Index of Prediction Accuracy) and non-proper (inverse probability censoring weights-adjusted concordance index) scoring rules. A simple simulation with an exponential hazard model shows CBNN consistently outperforming the other neural network methods. The complex simulation highlights the ability of CBNN to model both a complex baseline hazard and time-varying interactions, outperforming all competitors. The first real data application shows CBNN outperforming all neural network competitors, while a second real data application shows competitive performance. In the simulations and real data applications, CBNN provides a more consistent predictive performance across time than the competing neural network approaches.


# Introduction


what is aft, dont like assuming distribution, smooth. Less used than cox. Proportional hazards assumptions hard to maintain in long study, all regression require prior knowledge.



The widespread use of the Cox proportional hazards model has led to a focus on survival models based on hazard ratios and relative risks rather than on survival curves and absolute risks [@hanley2009].
In clinical settings, Cox proportional hazards models are frequently prioritized over smooth-in-time accelerated failure time (AFT) models [@hanley2009], which can estimate absolute risks by modeling the hazard directly through user specification of a baseline hazard distribution. Using splines as the baseline hazard is an alternative to assuming a specific distribution [@royston2002flexible]. The risk associated with each covariate may not be constant over time, making it difficult to maintain the proportional hazards assumption. Previous studies related to breast cancer have discovered time-varying interactions with covariates of interest, like tumor size [@coradini2000time]. A limitation of regression-based models is the requirement of prior knowledge; the analyst must know potential time-varying interactions and explicitly evaluate them.


Smooth-in-time accelerated failure time (AFT) models can estimate absolute risks by modeling the hazard directly through user specification of a baseline hazard distribution [@kleinbaum2012survival]. The assumption of a specific distribution tends to be a point of contention. A method has been developped using splines, providing flexibility to the baseline hazard  [@royston2002flexible]. AFT models are underutilized compared to Cox proportional hazards, resulting in a focus on survival models based on hazard ratios and relative risks rather than on survival curves and absolute risks [@hanley2009]. However, The risk associated with each covariate may not be constant over time, making it difficult to maintain the proportional hazards assumption. Previous studies related to breast cancer have discovered time-varying interactions with covariates of interest, like tumor size [@coradini2000time]. A limitation of all regression-based models is the requirement of prior knowledge; the analyst must know potential time-varying interactions and explicitly evaluate them.

Neural networks provide a data-driven approach to approximating interaction terms. DeepSurv is a neural network-based proportional hazards model that implements the Cox partial log-likelihood as a custom loss function [@katzman2018DeepSurv]. This results in a stepwise absolute risk curve that cannot accommodate time-varying interactions. Compared to Cox regression, DeepSurv has shown an improvement in performance on the Study to Understand Prognoses Preferences and Risk Treatments (SUPPORT) dataset [@knaus1995SUPPORT]. Farraggi et al. suggested a modification to the loss function required for a Cox neural network that can handle non-proportional hazards [@faraggi1995neural]. Wang et al. took the concept of extreme learning machines and applied them in a survival context, removing the need for tuning the number of layers and nodes [@wang2019extreme].

DeepHit requires that each survival time of interest be specified in the model and directly estimates survival curves, rather than deriving a hazard function [@lee2018DeepHit]. It assumes an inverse Gaussian distribution as the baseline hazard and its concordance index (c-index) score outperforms DeepSurv [@katzman2018DeepSurv] on the Molecular Taxonomy of Breast Cancer International Consortium (METABRIC) dataset [@curtis2012genomic].

Deep Survival Machines (DSM) provides a flexible, parametric survival model using neural networks with a mixture of distributions as the baseline hazard [@dsmPaper]. On both the SUPPORT and METABRIC datasets, DSM outperforms DeepSurv and DeepHit [@dsmPaper]. However, like DeepHit and DeepSurv, DSM cannot model time-varying interactions.

These methods need custom loss and activation functions that are difficult to implement in practice. We note the complexity of these alternative neural network approaches; DeepSurv, DeepHit, and DSM all require custom loss functions [@katzman2018DeepSurv] [@lee2018DeepHit] [@dsmPaper], while DSM requires a two-phase learning process and user implementations for distributions beyond Log-Normal or Weibull [@dsmPaper]. Regression-based approaches require prior specification of all interaction terms, which makes it challenging to model covariate effects that change over time. In this article, we propose Case-Base Neural Networks (CBNN) as a method that models time-varying interactions and a flexible baseline hazard using commonly available neural network components. Our approach to modeling the full hazard uses case-base sampling [@hanley2009]. This sampling technique allows probabilistic models to predict survival outcomes. Following the case-base framework, we can use transformations of time as a feature (covariate) to specify different baseline hazards. For example, by including splines of time as covariates, we can approximate the Royston-Parmar flexible baseline hazard model [@royston2002flexible], however, this still requires user specification of time-varying interactions. CBNN can model both without user specification; only a sufficiently deep network is required.

In Section \ref{methods}, we describe how case-base sampling and neural networks are combined both conceptually and algebraically, along with our hyperparameter choices and software implementation. In Section \ref{sims}, we describe our metrics and compare the performance of CBNN, DeepSurv, DeepHit, DSM, Cox regression, and case-base using logistic regression on simulated data. Section \ref{casestudies} describes the real-data analysis, while in Section \ref{discussion}, we explore the implications of our results and contextualize them within neural network survival analysis in a single event setting.

# Case-base neural network methodology, metrics, and software {#methods}

In this section, we define case-base sampling, which converts the total survival time into discrete person-specific moments (person-moments). Then, we detail how neural networks can be used within this framework, explicitly incorporating time as a feature while adjusting for the sampling bias. Finally, we report on the software versions used. A package is available for use at [GITHUBLINK]. The entire code base to generate the analyses in this paper is available at [GITHUB LINK].

## Case-base sampling 

Case-base sampling is an alternative framework for survival analysis [@hanley2009]. In case-base sampling, we sample from the continuous survival time of each person in our dataset to create a *base series* of *person-moments*. This *base series* complements the *case series*, which contains all person-moments at which the event of interest occurs.

For each person-moment sampled, let $X_i$ be the corresponding covariate profile $\left(x_{i1},x_{i2},...,x_{ip} \right)$, $T_i$ be the time of the person-moment, and $Y_i$ be the indicator variable for whether the event of interest occurred at time $T_i$. We estimate the hazard function $h(t \mid X_i)$ using the sampled person-moments. Recall that $h(t \mid X_i)$ is the instantaneous potential of experiencing the event at time $t$ for a given set of covariates $X_i$, assuming $T_i \geq t$.

Now, let $b$ be the (user-defined) size of the *base series*, and let $B$ be the sum of all follow-up times for the individuals in the study. If we sample the *base series* uniformly, then the hazard function of the sampling process is equal to $b/B$. Therefore, we have the following equality \footnote{We are abusing notation here, conflating hazards with probabilities. For a rigorous treatment, see Saarela \& Hanley (2015) section 3 \cite{saarela2015} .}: \begin{align}\label{eqn:main}
\frac{P\left(Y_i=1 \mid X_i, T_i\right)}{P\left(Y_i = 0 \mid X_i, T_i\right)} = \frac{h\left(T_i \mid X_i\right)}{b/B}.
\end{align} 
The odds of a person-moment being a part of the *case series* is the ratio of the hazard $h(T_i \mid X_i)$ and the uniform rate $b/B$. Using \eqref{eqn:main}, we can see how the log-hazard function can be estimated from the log-odds arising from case-base sampling: \begin{align}\label{eqn:offset}
\log \left( h\left(t \mid X_i\right)\right) = \log \left(\frac{P\left(Y_i = 1 \mid X_i, t\right)}{P\left(Y_i = 0 \mid X_i, t\right)}\right) + \log\left(\frac{b}{B}\right).
\end{align}

We may estimate the hazard function if we adjust for the bias introduced when sampling a fraction of the study base $B$ by adding the offset term $\log\left(\frac{b}{B} \right)$ (\eqref{eqn:offset}). Next, we propose using neural networks to model the odds.


## Neural networks to model the hazard function

After case-base sampling, we pass all features, including time, into any user-defined feed-forward component, to which an offset term (to adjust for bias from case-base sampling) is added, then passed through a sigmoid activation function (Figure \ref{fig:NNarch}). We use the sigmoid function as its inverse is the odds, which we can use to calculate the hazard. The general form for the neural network using CBNN is:

\begin{align}\label{eqn:nnProb}
P\left(Y=1|X,T\right)&=\sigmoid\left(f_{\theta}(X, T) + \log\left(\frac{B}{b}\right) \right),
\end{align}

where $T$ is a random variable representing the event time, $X$ is the random variable for a covariate profile, $f_{\theta}(X, T)$ represents any feed-forward neural network architecture, $\log\left(\frac{B}{b}\right)$ is bias term set by case-base sampling, $\theta$ is the set of parameters learned by the neural network and $\sigmoid(x)=\frac{1}{1+e^{-x}}$. By allowing a neural network to approximate a higher-order polynomial of time, the baseline hazard specification is now data-driven, where user-defined hyperparameters such as regularization, number of nodes, and layers control the flexibility of the model. We provide a detailed description of the choices we made in the next sub-section.

```{r NNarch,echo = FALSE, fig.align = 'center', out.width = "100%", fig.cap = "Steps involved in CBNN from case-base sampling to the model framework we use for training. The first step is case-base sampling, completed before training begins. Next, we pass this sampled data through a feed-forward neural network. We add the offset and pass that through a sigmoid activation function, whose output is a probability. Once the neural network model completes its training, we can convert the probability output to a hazard, using it for our survival outcomes of interest."}
library(knitr)
knitr::include_graphics(here::here("figures", "nnarch2.PNG"))
```

The following derivation shows how our probability estimate is converted to odds:
\begin{align*}
 \log\left( h(t \mid X_i) \right) &= \log\left(\frac{\sigmoid \left(f_{\theta}(X, T) + \log\left(\frac{B}{b}\right)\right)}{1-\sigmoid \left(f_{\theta}(X, T) + \log\left(\frac{B}{b}\right)\right)}\right) + \log\left(\frac{b}{B}\right) \\
 &= \log\left( \frac{\frac{\exp\left(f_{\theta}(X, T) + \log\left(\frac{B}{b}\right)\right)}{\exp\left(f_{\theta}(X, T) + \log\left(\frac{B}{b}\right)\right)+1}}{1-\frac{\exp\left(f_{\theta}(X, T) + \log\left(\frac{B}{b}\right)\right)}{\exp\left(f_{\theta}(X, T) + \log\left(\frac{B}{b}\right)\right)+1}}\right) + \log\left(\frac{b}{B}\right) \\
 &= \log\left(\exp\left( f_{\theta}(X, T) + \log\left(\frac{B}{b}\right) \right) \right) + \log\left(\frac{b}{B}\right) \\
 &= f_{\theta}(X, T) + \log\left(\frac{B}{b}\right) + \log\left(\frac{b}{B}\right) \\
&= f_{\theta}(X, T). 
\end{align*}

We use binary cross-entropy as our loss function [@gulli2017]:
\begin{align*}
L(\theta)=-\frac{1}{N} \sum^{N}_{i=1} y_{i} \cdot \log(\hat{f}_{\theta}(x_{i}, t_{i}) ) + (1-y_{i} )\cdot \log(1-\hat{f}_{\theta}(x_{i}, t_{i}) ),
\end{align*}
where $\hat{f}_{\theta}(x_{i}, t_{i})$ is our estimate for a given covariate profile and time, $y_{i}$ is our target value specifying whether an event occurred, and $N$ represents the number of individuals in our training set. 

Backpropagation is used to optimize the parameters in the model with an appropriate minimization algorithm (e.g. Adam, RMSPropagation, stochastic gradient descent)[@gulli2017]. For our analysis, we use Adam. Note that the size of the *case series* is fixed as the number of events, but we can make the *base series* as large as we want. A ratio of 100:1 *base series* to *case series* is sufficient [@hanley2009]. We pass our feed-forward neural network through a sigmoid activation function (Figure \ref{fig:NNarch}). Finally, we can convert this model output to a hazard. When using our model for predictions, we manually set the offset term to 0 in the new data, as we account for the bias during the fitting process.

Since we are directly modeling the hazard, we can readily estimate the risk function ($F$) at time $t$ for a covariate profile $X$, viz.\begin{align}\label{eqn:ci2}
F\left(t|X\right)& = 1 - \exp\left(-\int_{0}^{t}h(u|X) \,\textrm du\right).
\end{align} 
We use Riemann's Sum [@hughes2020calculus] to approximate the integral in \eqref{eqn:ci2}.


## Hyperparameter selection

Neural networks provide flexibility when defining the architecture and optimization parameters of our models. These hyperparameter choices can affect the estimated parameters and were chosen during a set of initial simulations to determine if CBNN can learn complex interactions in practice. We set a max number of epochs to be $2000$, batch size as $512$, learning rate as $10^{-3}$, decay as $10^{-7}$, patience as $10$ epochs, $\{50,50,25,25\}$ nodes in each hidden layer with $50\%$ dropout at each layer, a validation split of $20\%$, a testing split of $10%$, a minimum delta loss on the validation set of $10^{-7}$ over 10 epochs, and Adam [@gulli2017] as our optimizer. These choices may permit the model to approximate higher-order interactions while preventing over-fitting [@srivastava2014dropout]. We fix a train-validation-test split that allows us to update the weights with a subset of the data (training), assess performance at each epoch (validation) and gauge performance of the final model (test). We select the best weights after training based on validation loss [@gulli2017].

## Software implementation

R [@Rsoft] and python [@py] are used to evaluate methods from both languages. We fit the Cox model using the **survival** package [@survpkg], the CBLR model using the **casebase** package [@cbpkg], the DeepSurv model using the **survivalmodels** package [@survmods], the DSM model using **DeepSurvivalMachines** [@dsmPaper] and the DeepHit model using **pycox** [@lee2018DeepHit]. We made the components of CBNN using the **keras** [@keras] package and the **casebase** package for the sampling step. The **simsurv** package [@simsurv] is used to simulate our simulation studies, while **flexsurv** [@flexsurv] is used to fit a flexible baseline hazard using splines for our complex simulation. We use the implementation of c-ipcw from the python package **sksurv** [@sksurv]. The **riskRegression** package [@riskRegression] is used to get the Brier score and IPA metrics. We changed the **riskRegression** package to be used with any user supplied risk function $F$. To ensure that both R and Python-based models are running in unison on the same data through our simulations and bootstrap, we use the **reticulate** package [@reticulate]. 

# Simulation studies {#sims}

In this section, we use simulated data to evaluate the performance of CBNN and compare our approach with existing methods based on regression (Cox, CBLR) and neural network (DeepHit, DeepSurv, DSM) methods. We specify a linear combination of each covariate as the linear predictor in regression-based approaches (Cox, CBLR), which contrasts with neural network approaches that allow for non-linear interactions. We simulate data under a simple exponential model, and a complex baseline hazard with time-varying interactions, each with 10% random censoring. For both settings, we simulate three covariates:

$$
z_{1} \sim \textrm{Bernoulli}(0.5) \qquad \qquad
z_{2} \sim \begin{cases}
 N(0,0.5) & \textrm{if } z_{1}=0\\ 
 N(10,0.5) & \textrm{if } z_{1}=1
\end{cases} \qquad \qquad
z_{3} \sim \begin{cases}
 N(8,0.5) & \textrm{if } z_{1}=0\\ 
 N(-3,0.5) & \textrm{if } z_{1}=1.
\end{cases}
$$

The DeepHit-specific hyperparameter alpha is set to $0.5$ (equal weight between its negative log-likelihood and ranking losses [@lee2018DeepHit]). We change the **DeepSurvivalMachines** [@dsmPaper] package to include dropout and a minimum delta loss during the fitting process. For DSM, we define a mixture model of six Weibull distributions for the baseline hazard. All other hyperparameters are held constant across all neural network methods in both the simulation studies and real data applications.

Besides the methods mentioned above, we include the Optimal model in our comparisons using CBLR. That is, we include the exact functional form of the covariates in a CBLR model (referred to as Optimal for simplicity). We calculate $t$-based 95% confidence intervals using 100 replications of the simulated data For all analyses, we use 80% for training and 20% for the test set. 20% of the training set is kept for validation at each epoch. We predict risk functions $F$ using \eqref{eqn:ci2} for individuals in the test set, which are used to calculate our c-ipcw and IPA scores.

## Performance metrics

We use two metrics to assess the performance of the different methods of interest: 1) index of prediction accuracy (IPA) [@kattan2018index] and 2) inverse probability censoring weights-adjusted concordance index (c-ipcw) [@uno2011], which we define below. Both these time-dependent metrics provide transparency as to when in follow-up time each model may perform better than the others.

### Index of prediction accuracy (IPA)

The IPA is a function of the Brier score ($BS(t)$) [@graf1999], which is defined as
\begin{align}
BS(t)=\frac{1}{N}\sum^{N}_{i=1}\left(\frac{\left(\widehat{F}(X_{i},t)-1\right)^{2}\cdot I_{T_{i}\leq t,\delta_{i}=1}}{\widehat{G}(T_{i})} + \frac{\left(\widehat{F}(X_{i},t)\right)^{2}\cdot I_{T_{i}>t}}{\widehat{G}(t)}\right),
\end{align}
where $\delta_{i}=1$ shows individuals who have experienced the event, $N$ represents the number of samples in our dataset over which we calculate $BS(t)$, $\widehat{G}(t)=P[C>t]$ is a non-parametric estimate of the censoring distribution, $C$ is censoring time and $T_{i}$ is an individual’s survival or censoring time. The Brier score provides a score that accounts for the information loss because of censoring. There are three categories of individuals that may appear within the dataset once we fix our $t$ of interest. Individuals who experienced the event before $t$ are present in the first term of the equation. The second term of the equation includes individuals who experience the event or are censored after $t$. Those censored before $t$ are the third category of people. The  inverse probability censoring weights (IPCW) adjustment ($G(\cdot)$) is to account for these category three individuals whose information is missing. The IPA score as a function of time is given by
\begin{align}
\textrm{IPA}(t) &= 1-\frac{BS_{model}(t)}{BS_{null}(t)}, \nonumber
\end{align} where $BS_{model}(t)$ represents the Brier score over time $t$ for the model of interest, and $BS_{null}(t)$ represents the Brier score if we use an unadjusted Kaplan-Meier (KM) curve as the prediction for all observations [@kattan2018index]. Note that IPA has an upper bound of one, where positive values show an increase in performance over the null model and negative values show that the null model performs better. These scores demonstrate how performance changes over follow-up time.

A potential artifact of IPA is that the score is unstable at earlier and later survival times. This is because of a near equivalent Brier score between each model and the null model. At small values, a difference of $0.1$ creates a more significant fold change than at larger values. As the Brier score is potentially small at the start and at the end of their respective curves, we may see instability of the IPA score at those ends.

### Inverse probability censoring weights-adjusted concordance index

The c-ipcw is a non-proper, rank-based metric that does not depend on the censoring times in the test data [@uno2011]. The c-ipcw is given by 

\begin{align} \label{eq:cidx}
c-ipcw(t) &= \frac{\sum^{N}_{i=1}\sum^{N}_{j=1}\delta_{i}\left\{\widehat{G}(T_{i})\right\}^{-2} I(T_{i}<T_{j},T_{i}<t) I\left(\widehat{F}(t|X_{i})>\widehat{F}(t|X_{j})\right)}{\sum^{N}_{i=1}\sum^{N}_{j=1}\delta_{i}\left\{\widehat{G}(T_{i})\right\}^{-2} I(T_{i}<T_{j},T_{i}<t)},
\end{align} where the follow-up period of interest is (0,$t$), $I(\cdot)$ is an indicator function, $\widehat{F}(X_{i},t)$ is the risk function estimated for everyone in the study at time $t$ and c-ipcw can compare the performance of different models, where a higher score is better. Note that the c-ipcw may produce misleading performance, as it ranks based on survival times, not event status [@cindexfails2019]. This metric is considered an unbiased population concordance measure because of the IPCW adjustment [@uno2011].



\srb{in figure 2 i changed $C_{IPCW}$ to c-ipcw, as capital C is used for censoring. in the plots, would it make sense to leave it as a small letter?}

```{r megaPlot, echo = FALSE , fig.align = 'center', out.width = "100%", fig.cap = "Summarizes the simple simulation (A, B), complex simulation (C, D), SUPPORT case study (E, F) and METABRIC case study (G, H) results. The first row shows the IPA score for each model in each study over follow-up time. Negative values mean our model performs worse than the null model and positive values mean the model performs better. The second row shows the c-ipcw for each model in each study over follow-up time. A score of 1 is the maximum performance for either metric. Each model-specific metric in each study shows a 95-percent confidence interval over 100 iterations. The models of interest are case-base with logistic regression (CBLR), Case-Base Neural Networks (CBNN), Cox, DeepHit, DeepSurv, Deep Survival Machines (DSM), Optimal (a CBLR model with the exact interaction terms and baseline hazard specified) and Kaplan-Meier (to serve as a baseline, predicting the average for all individuals)."}
#readRDS(here::here("figures", "megaPlot.rds"))
knitr::include_graphics(here::here("../pmnnPlayground/reviewedAnalysisPMNN/figures", "megaPlot.pdf"))
```




## Simple simulation: constant baseline hazard

We simulate data from a simple model that depends on a constant baseline hazard:
$$h(t \mid X_i) = \lambda\cdot e^{\beta_{{1}}z_{1}+\beta_{{2}}z_{2}+\beta_{{3}}z_{3}},$$
with minimal covariate effects given by $\beta_{{1}}=0.1, \beta_{{2}}=0.1, \beta_{{3}}=0.1$ and $\lambda=1.0$. Once we simulate survival times, we introduce 10% random censoring. 

### Performance comparison in simple simulation

Figure \ref{fig:megaPlot} A, B and Table \ref{tab:megaTable} A show the results for the simple simulation. The regression-based methods (CBLR, Cox, Optimal) outperform the neural network ones in the simple simulation setting. Among the neural network approaches, CBNN outperforms all other methods in terms of both IPA and c-ipcw (Figure \ref{fig:megaPlot} A, B). Specifically, we see CBNN is consistent across time with smaller confidence intervals compared to DeepHit, DeepSurv and DSM.

In this simple setting, the regression models are much closer to the Optimal model, while the neural network models perform worse than the null model. The wide confidence bands in c-ipcw suggest the neural network models may be over-parameterized.



## Complex simulation: flexible baseline hazard, time-varying interactions

This simulation demonstrates performance with the presence of a complex baseline hazard and a time-varying interaction. Originally used to demonstrate the spline-based hazard model proposed by Royston and Parmar [@royston2002flexible], the breast cancer dataset provides a complex hazard from which we simulate, available in the **flexsurv** R package [@flexsurv]. To increase the complexity of our data-generating mechanism for this simulation, we design the model as follows:
\begin{align}
h(t \mid X_i) =\sum_{i=1}^{5} (\gamma_{i} \cdot \psi_{i}) + \beta_{{1}} (z_{1}) + \beta_{{2}} (z_{2})+ \beta_{{3}} (z_{3})+ \tau_{1} ( z_{1} \cdot z_{2} \cdot t)+ \tau_{2} ( z_{1} \cdot z_{3})+ \tau_{3} (z_{2} \cdot z_{3}), \nonumber
\end{align}



```{=latex}
\FloatBarrier
\begin{table}
\caption{Four tables representing performance at certain follow-up times for the simple simulation, complex simulation, SUPPORT and METABRIC. Each table shows performance for each method in each study at $25\%$, $50\%$. $75\%$ and $100\%$ of follow-up time. The bold elements show the best model for each study, at each follow-up time of interest. These tables are included to provide exact measures at certain intervals. The models of interest are: Cox, case-base with logistic regression (CBLR), DeepSurv, DeepHit, Case-Base Neural Networks (CBNN), Optimal (a CBLR model with the exact interaction terms and baseline hazard specified) and Deep Survival Machines (DSM).}
\label{tab:megaTable}
```

```{r megaTable, echo = FALSE , fig.align = 'center', out.width = "100%"}
#readRDS(here::here("figures", "megaPlot.rds"))
knitr::include_graphics(here::here("../pmnnPlayground/reviewedAnalysisPMNN/figures", "megaTable.pdf"))
```

```{=latex}
\end{table}
\FloatBarrier
```


where $\gamma_{1}=3.9, \gamma_{2}=3, \gamma_{3}=-0.43, \gamma_{4}=1.33,\gamma_{5}=-0.86, \beta_{{1}}=1, \beta_{{2}}=1, \beta_{{3}}=1, \tau_{1}=10, \tau_{2}=2, \tau_{3}=2$ and $\psi$ are basis splines. To generate the $gamma$ coefficients, let $g$: $\mathbb{R} \rightarrow \mathbb{R}$ be a smoothing method for variable $t$ by a projection on to a set of basis functions on splines: 
\srb{i used gt as the function for splines. Specifically, natural cubic spline function of log time https://www.rdocumentation.org/packages/flexsurv/versions/2.1/topics/flexsurvspline . therefore, should i use g(log(t)) and and replace "smoothing method" with natural cubic spline? SAHIR: will fix any issues in this section in the final version.}
\begin{align}
g(t)=\sum^{m}_{l=1}\psi_{l} (t)\gamma_{l}.
\end{align} 


An initial model with three knots ($m=5$) is fit using the *flexsurvspline* function with an intercept-only model on the data, which returns the coefficients ($\gamma$) and basis of time ($\psi$) we use in our model. Note that we fix these values for the analysis. The $\beta$ coefficients represent direct effects, $\tau_{2}$ and ${\tau_3}$ represent interactions and $\tau_{1}$ is a time-varying interaction.


### Performance comparison in complex simulation

Figure \ref{fig:megaPlot} C, D and Table \ref{tab:megaTable} B show the performance over time on a test set in the complex simulation. Apart from the Optimal regression model, CBNN outperforms the competing models when examining IPA, and up to the 75-percentile of follow-up time for c-ipcw. We expected the Optimal model to perform best in both metrics. However, this was not the case for c-ipcw and may be due to an artifact of concordance-based metrics, where a misspecified model may perform better than a correctly specified one [@cindexfails2019]. We attribute this difference in performance to the flexibility of CBNN in terms of time-varying interactions and baseline hazard, flexibility the other neural network models do not have, explaining their drop in performance.

# Application to SUPPORT and METABRIC data {#casestudies}

Our complex simulation demonstrates the superior performance of CBNN in ideal conditions with clean data. To obtain a more realistic performance assessment, we compared models using two real datasets with a time-to-event outcome. The first case study examines the SUPPORT dataset [@knaus1995SUPPORT]. The second case study examines the METABRIC dataset [@curtis2012genomic]. We use the same hyperparameters as in the simulation studies. As we do not know the true model for the real data, we exclude the Optimal model. We split the datasets keeping 20% of the observations as a test set. 20% of the training set is kept aside for validation at each epoch. We predict risk functions for everyone in the test set, which are used to calculate our metrics. We conduct 100 bootstrap re-samples for the real data applications to obtain confidence intervals.

## Performance evaluation using the SUPPORT dataset


The SUPPORT dataset tracks the time until death for seriously ill patients at five American hospitals [@knaus1995SUPPORT]. We use a pre-processed version of the dataset made available in the DeepSurv package [@katzman2018DeepSurv]. This dataset contains 9104 samples and 14 covariates (age, sex, race, number of comorbidities, presence of diabetes, presence of dementia, presence of cancer, mean arterial blood pressure, heart rate, respiration rate, temperature, white blood cell count, serum’s sodium, and serum’s creatinine) [@katzman2018DeepSurv]. Patients with missing features were excluded and 68.10% of the patients died during the 5.56-year study period [@katzman2018DeepSurv].

Figure \ref{fig:megaPlot} E, F and Table \ref{tab:megaTable} C demonstrates the performance over time on a test set. The regression models (CBLR, Cox) perform best considering IPA, followed by CBNN from the $25^{th}$ to $100^{th}$ percentile of follow-up time. We note a drop in performance for CBNN before the $25^{th}$ percentile of follow-up. For c-ipcw, CBNN outperforms the competing models consistently over follow-up time. Note that performance is similar for all models, aside from DeepSurv whose c-ipcw is lower than the rest (Figure \ref{fig:megaPlot} E, F and Table \ref{tab:megaTable} C).


## Performance evaluation using the METABRIC dataset

METABRIC is a 30-yearlong study aiming to discover the molecular drivers of breast tumors, following 2000 individuals with breast cancer until death [@curtis2012genomic]. They described these growths as primary invasive breast carcinomas, with the goal of discovering both genetic and clinical risk factors for breast cancer survival [@curtis2012genomic]. We used the processed dataset made available through DeepSurv [@katzman2018DeepSurv], which includes 1980 patients of which 57.72% die due to breast cancer within a median 10 years of follow-up [@katzman2018DeepSurv]. There are 11 covariates in total: 4 RNA-Seq gene expressions (MKI67, EGFR, PGR, and ERBB2) and 5 clinical features (hormone treatment indicator, radiotherapy indicator, chemotherapy indicator, ER-positive indicator, and age at diagnosis) [@katzman2018DeepSurv].

Figure \ref{fig:megaPlot} G, H and Table \ref{tab:megaTable} D shows the performance on a test set over time. The IPA scores suggest that regression models outperform competing models on this dataset, as all the neural network models are equal to or perform worse than KM over follow-up time. Our CBNN model is comparable to KM until around the 50-percentile of follow-up time, after which CBNN, DeepSurv and DSM drop in performance. The c-ipcw produces a different ranking. Our CBNN model outperforms the other models up to the $25^{th}$ percentile of follow-up time, whereas DeepHit performs best from the $50^{th}$ to $75^{th}$ percentile of follow-up. With this metric, CBNN and DeepHit outperform the regression models. This disagreement between IPA and c-ipcw between may be due to the misspecification issue of concordance-based metrics [@cindexfails2019]. The neural network models may be over-parameterized, as shown by the wide confidence bands in c-ipcw.


# Discussion {#discussion}

CBNN models survival outcomes by using neural networks on case-base sampled data. While estimating the hazard function, we incorporate follow-up-time as a feature, providing a data-driven estimation of a flexible baseline hazard and time-varying interactions. Based on our complex simulation results (Figure \ref{fig:megaPlot} C, D and Table \ref{tab:megaTable} B), CBNN outperforms the competitors when time-varying interactions and a complex baseline hazard are present.

The competing neural network model we evaluated can not model time varying interactions. DeepSurv is a proportional hazards model and does not estimate the baseline hazard [@katzman2018DeepSurv]. DeepHit requires an alpha hyperparameter, is restricted to a single distribution for the baseline hazard, and models the survival function directly [@lee2018DeepHit]. DSM requires the user to specify the component distributions in the mixture for a flexible baseline hazard to be fit [@dsmPaper]. These alternative neural network approaches match on time, while CBNN models time directly. 

In the simple simulation, all neural network-based approaches performed worse than KM (IPA scores), while the regression-based approaches did not. We attribute this to potential over-parameterization in the neural network models. The wide confidence intervals suggest over-fitting may be the reason for the loss in performance, even with dropout. Both DeepSurv and DSM are affected, while DeepHit and CBNN are less so.

In the complex simulation, CBNN has a distinct advantage over the other methods while examining the IPA score. The regression models do show improvement over the null model, while the competing neural network approaches do not. The time-varying interactions and complex baseline hazard may have caused a drop in performance for the competing neural network methods, compared to the regression models without interactions. 

In the SUPPORT application, the overall performance considering IPA is near identical. The regression models outperform CBNN, which performs better than the competing neural network models. As the confidence intervals are tight, it is unlikely that the loss in performance is because of over-fitting. Flexibility in both interaction modeling and baseline hazard is not particularly helpful with the SUPPORT dataset. 

In the METABRIC application IPA results, DSM and DeepSurv had a steep drop in performance. As DeepSurv does not depend on the baseline hazard during the fitting process and DSM is the most flexible competing model in our comparison, the baseline hazard is unlikely to cause this drop. With tight confidence intervals, over-fitting is unlikely as well. Further research would be required to determine the cause of this drop in performance. We do not see the same drop in CBNN demonstrating that, in the METABRIC application, CBNN outperforms DeepSurv and DSM. Considering IPA score, regression models consistently outperform the neural network models. From a biological perspective, the 30-year follow-up time in the METABRIC study may contain competing causes of death. The signal from the start of the study may not match the signal towards the end, potentially explaining the drop in performance as we reach later survival times with fewer individuals.

Concordance-based measures are commonly used in survival analysis to compare models and we opt to keep them in our analyses. However, c-ipcw is a non-proper metric and may cause misspecified models to appear better than they should [@cindexfails2019]. The model rankings between c-ipcw and IPA differed for both the complex simulation and METABRIC application. In the complex simulation, the c-ipcw of a correctly specified model (Optimal) underperforms CBNN and CBLR; models without the exact interaction terms specified. We base our METABRIC assessment of this disagreement between metrics on the complex simulation. The IPA score ranks regression models first, followed by DeepHit and CBNN, but the c-ipcw score ranks regression models after DeepHit and CBNN. Wide confidence intervals for neural network models in c-ipcw show potential misspecification, suggesting that regression models perform best in the METABRIC application.

During our testing for the complex simulation and SUPPORT application, DSM did not converge with our choice of hyper-parameters and model design. As the goal is to fix the design and compare performance, we did not change any shared hyperparameters. For DSM, a different number and combination of component distributions did not help the model converge, though we did not conduct an exhaustive test. This may be a limitation to their method or early-stage implementation issues. We include this method where it did function as it shares a similar goal of a flexible baseline hazard.

If the user suspects that there may be unknown time-varying interactions and a complex baseline hazard, CBNN should strongly be considered. Once we perform case-base sampling and adjust for the sampling bias, we can use a sigmoid activation function to predict our hazard function. The user can approach their problem as they would for probabilistic modeling while interpreting their prediction in a survival context. In terms of performance, we saw that CBNN consistently performs better than the other neural network-based approaches over follow-up time. This may be because of the hyperparameters chosen, as they differ from each of the respective methods papers. CBNN only requires the use of standard components in machine learning libraries (add layer and sigmoid activation function). Due to the simplicity in its implementation and by extension user experience, we expect CBNN to be both a user-friendly approach to data-driven survival analysis and easily extendable to any feed-forward neural network frameworks.

Though CBNN can model time-varying interactions and a flexible baseline hazard, all models require substantial amounts of data to learn real, complex relationships effectively. Larger datasets will appear with time, but the cost of generating longitudinal datasets remains. A viable alternative strategy is the use of transfer learning, where independent and identically distributed data can function as a preliminary model to learn related features in a parallel dataset, initializing weights for our survival task [@bozinovski2020reminder].

This paper describes CBNN in the single event setting. A next step would be to extend this approach to competing risks. A promising use case would be high-dimensional data with complex correlation structures, such as imaging data.

