\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\emailauthor{jesse.islam@mail.mcgill.ca}{Jesse Islam}
\emailauthor{turgeon.maxime@gmail.com}{Maxime Turgeon}
\emailauthor{rob.sladek@mcgill.ca}{Robert Sladek}
\emailauthor{sahir.bhatnagar@gmail.com}{Sahir Bhatnagar}
\providecommand \oddpage@label [2]{}
\Newlabel{inst1}{a}
\Newlabel{inst2}{b}
\Newlabel{inst3}{c}
\Newlabel{inst4}{d}
\citation{cox1972regression}
\citation{hanley2009}
\citation{hanley2009}
\citation{gao2006time}
\citation{zhu2022time}
\citation{na2020time}
\citation{coradini2000time}
\citation{salmon2023clinical}
\citation{royston2002flexible}
\citation{hanley2009}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{2}{section.1}\protected@file@percent }
\newlabel{introduction}{{1}{2}{Introduction}{section.1}{}}
\citation{zadeh2020DeepSurvnet}
\citation{wang2021gpdbn}
\citation{jia2023dccafn}
\@writefile{toc}{\contentsline {section}{\numberline {2}Related works}{3}{section.2}\protected@file@percent }
\newlabel{related}{{2}{3}{Related works}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Survival models that ignore censoring}{3}{subsection.2.1}\protected@file@percent }
\citation{katzman2018DeepSurv}
\citation{ds1}
\citation{ds2}
\citation{ds3}
\citation{she2020development}
\citation{ching2018cox}
\citation{yin2022cox}
\citation{huang2020deep}
\citation{hao2021deep}
\citation{meng2022novel}
\citation{chen2023pathological}
\citation{zhu2017wsisa}
\citation{tang2023explainable}
\citation{kaynar2023pideel}
\citation{barbieri2022predicting}
\citation{hao2022joint}
\citation{mobadersany2018predicting}
\citation{li2022hfbsurv}
\citation{chen2020pathomic}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Cox partial log-likelihood}{4}{subsection.2.2}\protected@file@percent }
\newlabel{coxFrame}{{2.2}{4}{Cox partial log-likelihood}{subsection.2.2}{}}
\citation{lee2018DeepHit}
\citation{gensheimer2019scalable}
\citation{giunchiglia2018rnn}
\citation{kopper2022deeppamm}
\citation{wulczyn2020deep}
\citation{vale2021long}
\citation{dsmPaper}
\citation{nagpal2021deep}
\citation{dsmPaper}
\citation{nagpal2021deep}
\citation{coradini2000time}
\citation{gao2006time}
\citation{na2020time}
\citation{zhu2022time}
\citation{salmon2023clinical}
\citation{cox1972regression}
\citation{RJ-2022-052}
\citation{RJ-2022-052}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Discrete-time}{5}{subsection.2.3}\protected@file@percent }
\@writefile{toc}{\contentsline {subsection}{\numberline {2.4}Fully parametric framework}{5}{subsection.2.4}\protected@file@percent }
\citation{hanley2009}
\@writefile{toc}{\contentsline {section}{\numberline {3}Case-base neural network, metrics, hyperparameters and software}{6}{section.3}\protected@file@percent }
\newlabel{methods}{{3}{6}{Case-base neural network, metrics, hyperparameters and software}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Case-base sampling}{6}{subsection.3.1}\protected@file@percent }
\newlabel{case-base-sampling}{{3.1}{6}{Case-base sampling}{subsection.3.1}{}}
\citation{hanley2009}
\citation{hanley2009}
\citation{mantel1}
\citation{hanley2009}
\citation{mantel1}
\citation{saarela2015}
\citation{saarela2016case}
\newlabel{eqn:main}{{1}{7}{Case-base sampling}{equation.3.1}{}}
\newlabel{eqn:offset}{{2}{7}{Case-base sampling}{equation.3.2}{}}
\@writefile{loa}{\contentsline {algocf}{\numberline {1}{\ignorespaces Pseudocode algorithm of the case-base sampling procedure.}}{8}{algocf.1}\protected@file@percent }
\newlabel{alg:1}{{1}{8}{Case-base sampling}{algocf.1}{}}
\citation{gulli2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Neural networks to model the hazard function}{9}{subsection.3.2}\protected@file@percent }
\newlabel{neural-networks-to-model-the-hazard-function}{{3.2}{9}{Neural networks to model the hazard function}{subsection.3.2}{}}
\citation{gulli2017}
\citation{kingma2014adam}
\citation{hanley2009}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Methodological steps involved in CBNN. The first step, case-base sampling, is completed before training begins. Then, we pass this sampled data through a feed-forward neural network\textcolor {red}{,} add an offset to adjust for the bias inherent in case-base sampling and apply a sigmoid activation function to \textcolor {red}{estimate} a probability. Once the neural network model completes its training, we can convert the probability to a hazard for the survival outcome of interest.}}{10}{figure.1}\protected@file@percent }
\newlabel{fig:NNarch}{{1}{10}{Methodological steps involved in CBNN. The first step, case-base sampling, is completed before training begins. Then, we pass this sampled data through a feed-forward neural network\textcolor {red}{,} add an offset to adjust for the bias inherent in case-base sampling and apply a sigmoid activation function to \textcolor {red}{estimate} a probability. Once the neural network model completes its training, we can convert the probability to a hazard for the survival outcome of interest}{figure.1}{}}
\citation{hughes2020calculus}
\citation{graf1999}
\citation{kattan2018index}
\citation{auc}
\citation{graf1999}
\citation{graf1999}
\newlabel{eqn:ci2}{{3}{11}{Neural networks to model the hazard function}{equation.3.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Performance metrics}{11}{subsection.3.3}\protected@file@percent }
\newlabel{performance-metrics}{{3.3}{11}{Performance metrics}{subsection.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.1}Brier Score (BS)}{11}{subsubsection.3.3.1}\protected@file@percent }
\newlabel{bs}{{3.3.1}{11}{Brier Score (BS)}{subsubsection.3.3.1}{}}
\newlabel{eqn:bs}{{4}{11}{Brier Score (BS)}{equation.3.4}{}}
\citation{kattan2018index}
\citation{graf1999}
\citation{kattan2018index}
\citation{auc}
\citation{auc}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.2}Integrated Brier Score (IBS)}{12}{subsubsection.3.3.2}\protected@file@percent }
\newlabel{integrated-brier-score-ibs}{{3.3.2}{12}{Integrated Brier Score (IBS)}{subsubsection.3.3.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.3}Index of prediction accuracy (IPA)}{12}{subsubsection.3.3.3}\protected@file@percent }
\newlabel{index-of-prediction-accuracy-ipa}{{3.3.3}{12}{Index of prediction accuracy (IPA)}{subsubsection.3.3.3}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {3.3.4}Inverse probability censoring weights-adjusted time-dependent area under the receiver operating characteristic curve ($AUC_{IPCW}$)}{12}{subsubsection.3.3.4}\protected@file@percent }
\newlabel{auc2}{{3.3.4}{12}{Inverse probability censoring weights-adjusted time-dependent area under the receiver operating characteristic curve (\texorpdfstring {$AUC_{IPCW}$}{AUC\_IPCW})}{subsubsection.3.3.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.4}Hyperparameter selection}{13}{subsection.3.4}\protected@file@percent }
\newlabel{hyperparameter-selection}{{3.4}{13}{Hyperparameter selection}{subsection.3.4}{}}
\citation{Rsoft}
\citation{py}
\citation{survpkg}
\citation{cbpkg}
\citation{lee2018DeepHit}
\citation{cbpkg}
\citation{keras}
\citation{simsurv}
\citation{flexsurv}
\citation{riskRegression}
\citation{reticulate}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Hyperparameters selected after three-fold cross-validated grid search along with the average IBS for each neural network model in the complex simulation (A), multiple myeloma (MM) case study (B), free light chain (FLC) case study (C) and prostate cancer (Prostate) case study (D).}}{14}{table.1}\protected@file@percent }
\newlabel{tab:wins}{{1}{14}{Hyperparameters selected after three-fold cross-validated grid search along with the average IBS for each neural network model in the complex simulation (A), multiple myeloma (MM) case study (B), free light chain (FLC) case study (C) and prostate cancer (Prostate) case study (D)}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.5}Software implementation}{14}{subsection.3.5}\protected@file@percent }
\newlabel{software-implementation}{{3.5}{14}{Software implementation}{subsection.3.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Performance of each model in the complex simulation (A, E), multiple myeloma (MM) case study (B, F), free light chain (FLC) case study (C, G) and prostate cancer (Prostate) case study (D, H). The first row shows the IPA for each model in each study over follow-up time. Negative values mean the model performs worse than the null model and positive values mean the model performs better. The second row shows the $AUC_{IPCW}$ for each model in each study over follow-up time, where higher is better. Each model-specific metric in each study shows a 95\% confidence interval over 100 iterations. Metrics are shown for six models: Case-Base with Logistic Regression (CBLR), Case-Base Neural Network (CBNN), Cox Proportional Hazard (Cox), DeepHit and DeepSurv. The Kaplan-Meier (KM) model serves as a baseline, predicting the average curve for all individuals. CBLR and Cox have near identical performance, resulting in curves that overlap. The Optimal model (a CBLR model with the exact interaction terms and baseline hazard specified) shows the best performance we can expect on the simulated data.}}{15}{figure.2}\protected@file@percent }
\newlabel{fig:megaPlot}{{2}{15}{Performance of each model in the complex simulation (A, E), multiple myeloma (MM) case study (B, F), free light chain (FLC) case study (C, G) and prostate cancer (Prostate) case study (D, H). The first row shows the IPA for each model in each study over follow-up time. Negative values mean the model performs worse than the null model and positive values mean the model performs better. The second row shows the $AUC_{IPCW}$ for each model in each study over follow-up time, where higher is better. Each model-specific metric in each study shows a 95\% confidence interval over 100 iterations. Metrics are shown for six models: Case-Base with Logistic Regression (CBLR), Case-Base Neural Network (CBNN), Cox Proportional Hazard (Cox), DeepHit and DeepSurv. The Kaplan-Meier (KM) model serves as a baseline, predicting the average curve for all individuals. CBLR and Cox have near identical performance, resulting in curves that overlap. The Optimal model (a CBLR model with the exact interaction terms and baseline hazard specified) shows the best performance we can expect on the simulated data}{figure.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {4}Simulation study}{15}{section.4}\protected@file@percent }
\newlabel{sims}{{4}{15}{Simulation study}{section.4}{}}
\citation{flexsurv}
\citation{royston2002flexible}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Complex simulation: flexible baseline hazard, time-varying interactions}{16}{subsection.4.1}\protected@file@percent }
\newlabel{complex-simulation-flexible-baseline-hazard-time-varying-interactions}{{4.1}{16}{Complex simulation: flexible baseline hazard, time-varying interactions}{subsection.4.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Performance comparison in complex simulation}{16}{subsection.4.2}\protected@file@percent }
\newlabel{performance-comparison-in-complex-simulation}{{4.2}{16}{Performance comparison in complex simulation}{subsection.4.2}{}}
\citation{coradini2000time}
\citation{salmon2023clinical}
\citation{myeloma}
\citation{survpkg}
\citation{myeloma}
\citation{myeloma}
\citation{mm2flc}
\citation{flc}
\citation{survpkg}
\citation{flc}
\citation{flc}
\@writefile{toc}{\contentsline {section}{\numberline {5}Case studies}{17}{section.5}\protected@file@percent }
\newlabel{casestudies}{{5}{17}{Case studies}{section.5}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Performance evaluation on multiple myeloma dataset}{17}{subsection.5.1}\protected@file@percent }
\newlabel{pe-multiplemyeloma}{{5.1}{17}{Performance evaluation on multiple myeloma dataset}{subsection.5.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Performance evaluation on free light chain dataset}{17}{subsection.5.2}\protected@file@percent }
\newlabel{pe-flc}{{5.2}{17}{Performance evaluation on free light chain dataset}{subsection.5.2}{}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Performance at percentages of follow-up time in the complex simulation (A), multiple myeloma (MM) case study (B), free light chain (FLC) case study (C) and prostate cancer (Prostate) case study (D). Each table shows performance for each method at $25\%$, $50\%$, $75\%$ and $100\%$ of follow-up time. The models of interest are case-base with logistic regression (CBLR), Cox, Case-Base Neural Network (CBNN), DeepHit, DeepSurv, and Optimal (in the complex simulation). The best score at each percent of follow-up time is highlighted in bold. If tied, then all tied values are highlighted.}}{18}{table.2}\protected@file@percent }
\newlabel{tab:megaTable}{{2}{18}{Performance at percentages of follow-up time in the complex simulation (A), multiple myeloma (MM) case study (B), free light chain (FLC) case study (C) and prostate cancer (Prostate) case study (D). Each table shows performance for each method at $25\%$, $50\%$, $75\%$ and $100\%$ of follow-up time. The models of interest are case-base with logistic regression (CBLR), Cox, Case-Base Neural Network (CBNN), DeepHit, DeepSurv, and Optimal (in the complex simulation). The best score at each percent of follow-up time is highlighted in bold. If tied, then all tied values are highlighted}{table.2}{}}
\citation{prostate}
\citation{asaur}
\citation{prostate}
\citation{katzman2018DeepSurv}
\citation{lee2018DeepHit}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.3}Performance evaluation on prostate cancer dataset}{19}{subsection.5.3}\protected@file@percent }
\newlabel{pe-prostate}{{5.3}{19}{Performance evaluation on prostate cancer dataset}{subsection.5.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{19}{section.6}\protected@file@percent }
\newlabel{discussion}{{6}{19}{Discussion}{section.6}{}}
\citation{katzman2018DeepSurv}
\citation{lee2018DeepHit}
\citation{srivastava2014dropout}
\citation{lee2018DeepHit}
\citation{dsmPaper}
\citation{interpret}
\citation{hanley2009}
\@writefile{toc}{\contentsline {section}{\numberline {7}Conclusions}{21}{section.7}\protected@file@percent }
\newlabel{sec5}{{7}{21}{Conclusions}{section.7}{}}
\citation{survpkg}
\citation{asaur}
\newlabel{data-and-code-availability-statement}{{7}{22}{Data and code availability statement}{section*.1}{}}
\@writefile{toc}{\contentsline {section}{Data and code availability statement}{22}{section*.1}\protected@file@percent }
\newlabel{acknowledgements}{{7}{22}{Acknowledgements}{section*.2}{}}
\@writefile{toc}{\contentsline {section}{Acknowledgements}{22}{section*.2}\protected@file@percent }
\bibstyle{elsarticle-harv}
\bibdata{manuscript}
\bibcite{keras}{{1}{2021}{{Allaire and Chollet}}{{}}}
\bibcite{barbieri2022predicting}{{2}{2022}{{Barbieri et~al.}}{{Barbieri, Mehta, Wu, Bharat, Poppe, Jorm and Jackson}}}
\bibcite{cbpkg}{{3}{2020}{{Bhatnagar et~al.}}{{Bhatnagar, Turgeon, Islam, Hanley and Saarela}}}
\bibcite{RJ-2022-052}{{4}{2022}{{Bhatnagar* et~al.}}{{Bhatnagar*, Turgeon*, Islam, Hanley and Saarela}}}
\bibcite{ds2}{{5}{2020}{{Bice et~al.}}{{Bice, Kirby, Bahr, Rasmussen, Saenz, Wagner, Papanikolaou and Fakhreddine}}}
\bibcite{auc}{{6}{2015}{{Blanche et~al.}}{{Blanche, Proust-Lima, Loubere, Berr, Dartigues and Jacqmin-Gadda}}}
\bibcite{simsurv}{{7}{2020}{{Brilleman et~al.}}{{Brilleman, Wolfe, Moreno-Betancur and Crowther}}}
\bibcite{chen2023pathological}{{8}{2023}{{Chen et~al.}}{{Chen, Cao, Li, Liu, Liu, Tian, Sun, Wang, Gao, Kang et~al.}}}
\bibcite{chen2020pathomic}{{9}{2020}{{Chen et~al.}}{{Chen, Lu, Wang, Williamson, Rodig, Lindeman and Mahmood}}}
\bibcite{ching2018cox}{{10}{2018}{{Ching et~al.}}{{Ching, Zhu and Garmire}}}
\bibcite{coradini2000time}{{11}{2000}{{Coradini et~al.}}{{Coradini, Daidone, Boracchi, Biganzoli, Oriana, Bresciani, Pellizzaro, Tomasic, Di~Fronzo and Marubini}}}
\bibcite{cox1972regression}{{12}{1972}{{Cox}}{{}}}
\bibcite{flc}{{13}{2012}{{Dispenzieri et~al.}}{{Dispenzieri, Katzmann, Kyle, Larson, Therneau, Colby, Clark, Mead, Kumar, Melton~III et~al.}}}
\bibcite{mm2flc}{{14}{2009}{{Dispenzieri et~al.}}{{Dispenzieri, Kyle, Merlini, Miguel, Ludwig, H{\'a}jek, Palumbo, Jagannath, Blad{\'e}, Lonial et~al.}}}
\bibcite{gao2006time}{{15}{2006}{{Gao et~al.}}{{Gao, Grunwald, Rumsfeld, Schooley, MacKenzie and Shroyer}}}
\bibcite{gensheimer2019scalable}{{16}{2019}{{Gensheimer and Narasimhan}}{{}}}
\bibcite{riskRegression}{{17}{2021}{{Gerds and Kattan}}{{}}}
\bibcite{giunchiglia2018rnn}{{18}{2018}{{Giunchiglia et~al.}}{{Giunchiglia, Nemchenko and van~der Schaar}}}
\bibcite{graf1999}{{19}{1999}{{Graf et~al.}}{{Graf, Schmoor, Sauerbrei and Schumacher}}}
\bibcite{gulli2017}{{20}{2017}{{Gulli and Pal}}{{}}}
\bibcite{hanley2009}{{21}{2009}{{Hanley and Miettinen}}{{}}}
\bibcite{hao2021deep}{{22}{2021}{{Hao et~al.}}{{Hao, Kim, Kwon and Ha}}}
\bibcite{hao2022joint}{{23}{2022}{{Hao et~al.}}{{Hao, Jing and Sun}}}
\bibcite{huang2020deep}{{24}{2020}{{Huang et~al.}}{{Huang, Johnson, Han, Helm, Cao, Zhang, Salama, Rizkalla, Yu, Cheng et~al.}}}
\bibcite{hughes2020calculus}{{25}{2020}{{Hughes-Hallett et~al.}}{{Hughes-Hallett, Gleason and McCallum}}}
\bibcite{flexsurv}{{26}{2016}{{Jackson}}{{}}}
\bibcite{jia2023dccafn}{{27}{2023}{{Jia et~al.}}{{Jia, Ren, Wu, Zhao, Qiang and Yang}}}
\bibcite{kattan2018index}{{28}{2018}{{Kattan and Gerds}}{{}}}
\bibcite{katzman2018DeepSurv}{{29}{2018}{{Katzman et~al.}}{{Katzman, Shaham, Cloninger, Bates, Jiang and Kluger}}}
\bibcite{kaynar2023pideel}{{30}{2023}{{Kaynar et~al.}}{{Kaynar, Cakmakci, Bund, Todeschi, Namer and Cicek}}}
\bibcite{ds3}{{31}{2019}{{Kim et~al.}}{{Kim, Lee, Kwon, Nam, Cha and Kim}}}
\bibcite{kingma2014adam}{{32}{2014}{{Kingma and Ba}}{{}}}
\bibcite{kopper2022deeppamm}{{33}{2022}{{Kopper et~al.}}{{Kopper, Wiegrebe, Bischl, Bender and R{\"u}gamer}}}
\bibcite{myeloma}{{34}{1997}{{Kyle}}{{}}}
\bibcite{lee2018DeepHit}{{35}{2018}{{Lee et~al.}}{{Lee, Zame, Yoon and Schaar}}}
\bibcite{li2022hfbsurv}{{36}{2022}{{Li et~al.}}{{Li, Wu, Li and Wang}}}
\bibcite{prostate}{{37}{2009}{{Lu-Yao et~al.}}{{Lu-Yao, Albertsen, Moore, Shih, Lin, DiPaola, Barry, Zietman, O’Leary, Walker-Corkery et~al.}}}
\bibcite{mantel1}{{38}{1973}{{Mantel}}{{}}}
\bibcite{meng2022novel}{{39}{2022}{{Meng et~al.}}{{Meng, Wang, Zhang, Zhang, Zhang, Zhang and Wang}}}
\bibcite{mobadersany2018predicting}{{40}{2018}{{Mobadersany et~al.}}{{Mobadersany, Yousefi, Amgad, Gutman, Barnholtz-Sloan, Vel{\'a}zquez~Vega, Brat and Cooper}}}
\bibcite{asaur}{{41}{2016}{{Moore}}{{}}}
\bibcite{na2020time}{{42}{2020}{{Na et~al.}}{{Na, Cho, Kim, Choi and Han}}}
\bibcite{nagpal2021deep}{{43}{2021a}{{Nagpal et~al.}}{{Nagpal, Jeanselme and Dubrawski}}}
\bibcite{dsmPaper}{{44}{2021b}{{Nagpal et~al.}}{{Nagpal, Li and Dubrawski}}}
\bibcite{Rsoft}{{45}{2021}{{R Core Team}}{{}}}
\bibcite{royston2002flexible}{{46}{2002}{{Royston and Parmar}}{{}}}
\bibcite{saarela2016case}{{47}{2016}{{Saarela}}{{}}}
\bibcite{saarela2015}{{48}{2015}{{Saarela and Hanley}}{{}}}
\bibcite{salmon2023clinical}{{49}{2023}{{Salmon and Melendez-Torres}}{{}}}
\bibcite{she2020development}{{50}{2020}{{She et~al.}}{{She, Jin, Wu, Deng, Zhang, Su, Jiang, Liu, Xie, Cao et~al.}}}
\bibcite{srivastava2014dropout}{{51}{2014}{{Srivastava et~al.}}{{Srivastava, Hinton, Krizhevsky, Sutskever and Salakhutdinov}}}
\bibcite{tang2023explainable}{{52}{2023}{{Tang et~al.}}{{Tang, Liu, Chen, Ma, Dong, Sun, Zhang, Li, Zheng, Yang et~al.}}}
\bibcite{survpkg}{{53}{2000}{{{Terry M. Therneau} and {Patricia M. Grambsch}}}{{}}}
\bibcite{reticulate}{{54}{2021}{{Ushey et~al.}}{{Ushey, Allaire and Tang}}}
\bibcite{vale2021long}{{55}{2021}{{Vale-Silva and Rohr}}{{}}}
\bibcite{py}{{56}{2009}{{Van~Rossum and Drake}}{{}}}
\bibcite{wang2021gpdbn}{{57}{2021}{{Wang et~al.}}{{Wang, Li, Wang and Li}}}
\bibcite{wulczyn2020deep}{{58}{2020}{{Wulczyn et~al.}}{{Wulczyn, Steiner, Xu, Sadhwani, Wang, Flament-Auvigne, Mermel, Chen, Liu and Stumpe}}}
\bibcite{yin2022cox}{{59}{2022}{{Yin et~al.}}{{Yin, Chen, Wu and Wei}}}
\bibcite{ds1}{{60}{2022}{{Yu et~al.}}{{Yu, Huang, Feng and Lyu}}}
\bibcite{zadeh2020DeepSurvnet}{{61}{2020}{{Zadeh~Shirazi et~al.}}{{Zadeh~Shirazi, Fornaciari, Bagherian, Ebert, Koszyca and Gomez}}}
\bibcite{interpret}{{62}{2021}{{Zhang et~al.}}{{Zhang, Ti{\v {n}}o, Leonardis and Tang}}}
\bibcite{zhu2022time}{{63}{2022}{{Zhu et~al.}}{{Zhu, Liu, Zhang, Li, Chen, Huang, Li, Yu, Xu and Qin}}}
\bibcite{zhu2017wsisa}{{64}{2017}{{Zhu et~al.}}{{Zhu, Yao, Zhu and Huang}}}
\gdef \@abspage@last{30}
