 can estimate absolute
risks by modeling the hazard directly through a user-specified baseline hazard
distribution \citep{kleinbaum2012survival}. 

















\hypertarget{inverse-probability-censoring-weights-adjusted-concordance-index}{%
\subsubsection{Inverse probability censoring weights-adjusted
concordance
index \(C_{IPCW}\)}\label{inverse-probability-censoring-weights-adjusted-concordance-index}}

The \(C_{IPCW}\) is a non-proper, rank-based metric that does not depend
on the censoring times in the test data \citep{uno2011}. The
\(C_{IPCW}\) is given by 

\begin{align} \label{eq:cidx}
C_{IPCW}(t) &= \frac{\sum^{N}_{i=1}\sum^{N}_{j=1}\delta_{i}\left\{\widehat{G}(T_{i})\right\}^{-2} I(T_{i}<T_{j},T_{i}<t) I\left(\widehat{F}(t|X_{i})>\widehat{F}(t|X_{j})\right)}{\sum^{N}_{i=1}\sum^{N}_{j=1}\delta_{i}\left\{\widehat{G}(T_{i})\right\}^{-2} I(T_{i}<T_{j},T_{i}<t)}.
\end{align} where the follow-up period of interest is (0,\(t\)),
\(I(\cdot)\) is an indicator function, \(\widehat{F}(X_{i},t)\) is the
risk function estimated for everyone in the study at time \(t\) and
\(C_{IPCW}\) can compare the performance of different models, where a
higher score is better. Note that the \(C_{IPCW}\) may produce
misleading performance, as it ranks based on survival times, not event
status \citep{cindexfails2019}. This metric is considered an unbiased
population concordance measure because of the IPCW adjustment
\citep{uno2011}.





 While examining the IPA score, the optimal
regression model and CBNN perform near identically, leading in performance followed by 
DeepHit, the linear regression models and DeepSurv. We expected the Optimal model to perform best in both
metrics. However, this was not the case for \(C_{IPCW}\) and may be due
to an artifact of concordance-based metrics, where a misspecified model
may perform better than a correctly specified one
\citep{cindexfails2019}. We attribute the performance of CBNN to its
flexibility in modeling time-varying interactions and baseline hazard,
flexibility the other neural network models do not have. We keep this misspecification issue
in mind while interpreting the \(C_{IPCW}\) results in the case studies.


With the IPA score, CBNN outperforms
The competing models, followed by the linear models, DeepSurv and DeepHit. We note that after
50\% of survival time, all models aside from CBNN perform substantially worse than a null model.
We note with \(C_{IPCW}\), the linear models and DeepSurv perform equivalently, followed by CBNN and
DeepHit.


With the IPA score, CBNN outperforms
The competing models, followed by the linear models and lastly DeepSurv and DeepHit for the first 60\% of follow-up time. 
After 60\% of follow-up time, DeepHit has similar performance to CBNN. For the \(C_{IPCW}\) score, CBNN outperforms all models,
followed by the linear models and finally DeepSurv. Note that for earlier ranges of survival time, DeepHit is the worst performing model. 
After 25\% of follow-up time, DeepHit is in second place. In the FLC dataset, CBNN has a consistent top ranking for both IPA and \(C_{IPCW}\).




 With the IPA score, the linear models
outperform the neural network ones, followed by CBNN and DeepHit, and
finally DeepSurv. For the \(C_{IPCW}\) score, the linear models, CBNN, DeepHit
perform near identically, side from DeepHit with an initial drop in performance.
DeepSurv performed substantially worse than the other models.


To assess performance among these models, we use both IPA and
\(C_{IPCW}\) metrics. Concordance-based measures are commonly used in
survival analysis to compare models and we opt to keep them in our
analyses. However, \(C_{IPCW}\) is a non-proper metric and may cause
misspecified models to appear better than they should
\citep{cindexfails2019}. \(C_{IPCW}\) is based on the original Concordance
Index, a metric designed for proportional hazards (where the event risk is
constant over time) \citep{uno2011}. As such, the rank order of individuals
is would not change with follow-up time \citep{uno2011}. In studies with time-varying interactions (non-proportional hazards),
it is unclear how \(C_{IPCW}\) would perform as rank order may change. 
The model rankings between \(C_{IPCW}\) and IPA differ for all but FLC. We can further investigate this phenomenon
in our simulation, as the true model is known. Considering figure \ref{fig:megaPlot} A, E and
Table \ref{tab:megaTable} A in conjunction with FIGURE REF POPTIME, we know that the Optimal model should perform
best and acts as both a benchmark and metric assessment. It is a regression
model with a flexible baseline hazard and the exact interaction and time-varying interaction
specified. The IPA score shows the Optimal model performing best, as expected. This gives us
confidence that the IPA score can be used to compare all the models. The \(C_{IPCW}\) score did not.
A more robust set of tests is required to determine if \(C_{IPCW}\) fails due to the time-varying interaction or
the baseline hazard.

With this interpretation of \(C_{IPCW}\) in mind,


In the complex simulation, we note that \(C_{IPCW}\) is unable to properly rank the Optimal model.
Further work is required to determine the cause of this, and we focus our assessment on the IPA score.


censoring weights-adjusted concordance index (\(C_{IPCW}\))
\citep{uno2011}, which we define below. 

We use the implementation of \(C_{IPCW}\) from the python
package \textbf{sksurv} \citep{sksurv}.



A potential artifact of IPA is that the score is unstable at earlier and
later follow-up times. This is because of a near equivalent BS
among each model and the null model. At small values, a difference of
\(0.1\) creates a more significant fold change than at larger values. The IPA may
be unstable for follow-up times where the BS is low.

